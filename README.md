# Challenges
Este mono-repositorio contiene la solución a varias pruebas técnicas, challenges, ya sea presentados en procesos de entrevista o como desafíos personales. Los challenges empresariales han sido anonimizados para respetar la privacidad, y se dejan los links si se encuentran a las fuentes (kaggle, universidades, blogs, etc). 

## Challenge 1 - Análisis de datos de Airbnb (kaggle nyc airbnb)

Tarea basada en [este challenge de kaggle](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) donde se proporciona un conjunto de datos de publicaciones de hospedajes en la plataforma Airbnb para la ciudad de Nueva York. El objetivo es demostrar conceptos prácticos de calidad y limpieza de datos, y construir un reporte por barrio de los precios de los hospedajes. Esta parte de la prueba se encuentra en el archivo [Airbnb.ipynb](./1%20-%20Kaggle%20NYC%20Airbnb/Airbnb.ipynb)

## Challenge 2 - Modelos predictivos de calidad de vino (UCI ML Wine Quality)

Tarea de análisis en un conjunto de datos de características físicas y químicas de vinos de Vinho Verde basada en [este paper de la UCI (University of California, Irvine)](https://archive.ics.uci.edu/ml/datasets/wine+quality). El objetivo es entrenar modelos de regresión para predecir la calidad del vino en función de sus características. Esta parte de la prueba se encuentra en el archivo [VinhoVerde.ipynb](./2%20-%20UCI%20ML%20Wine%20Quality/VinhoVerde.ipynb)

## Challenge 3 - Fine tuning DistilBERT para Squadv2 (HuggingFace Transformers)
<div align="center">
  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Agq5NpK7kY3MnGX08d9IjIu7WNhn2s_Z?usp=sharing)
  
</div>

Tarea de fine tuning de un modelo de lenguaje pre-entrenado para responder preguntas en base a un contexto. El modelo utilizado es [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) y el conjunto de datos es [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/). Esta parte de la prueba se encuentra en el enlace de colab, o respaldado en el archivo [DistilBERT.ipynb](./3%20-%20HuggingFace%20Transformers/DistilBERT.ipynb).